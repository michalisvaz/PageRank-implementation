{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank, Markov Chains and Numerical Linear Algebra\n",
    "\n",
    "--------------------------------\n",
    "* By: Michail Vazaios (Μιχαήλ Βαζαίος - p3170013)\n",
    "* E-mail: mich.vaz.99@gmail.com or p3170013@aueb.gr\n",
    "* Course: \"Arithmetic methods in Statistics\" (Αριθμητικές μέθοδοι στην Στατιστική)\n",
    "--------------------------------\n",
    "#### Outline of project:\n",
    "\n",
    "1. **Introduction**\n",
    "2. **Datasets**\n",
    "    1. Followers on Github\n",
    "    2. Philosopher Network\n",
    "3. **Mathematics of PageRank**\n",
    "4. **Implementation**\n",
    "5. **Experiments**\n",
    "6. **Comments on results & final thoughts**\n",
    "7. **Sources**\n",
    "\n",
    "## 1. Introduction\n",
    "--------------------------------\n",
    "\n",
    "PageRank is an Algorithm for measuring the importance of nodes in directed graphs (although it can be easily extended to work for undirected graphs as well). It is used by Google Search to rank webpages in their search engine results. Of course it is not the only algorithm used by Google for the task of ordering search results, as it is combined with other information retrieval techniques to achieve better results. However, it was the first method used by the company and it remains their most famous algorithm. \n",
    "\n",
    "This project aims to implement a simple version of PageRank and test it to measure the importance of nodes in some experimental graphs by comparing the PageRank scores of each node to other metrics of node importance such as In-Degree Centrality, Closeness Centrality, Betweenness Centrality and Eigenvector Centrality.\n",
    "\n",
    "Note: Throughout this project we use the terms of each of the pairs graph-network, vertex-node, edge-link interchangeably. Generally the words \"graph, vertex, edge\" are mostly used by mathematicians or when we focus on Graph Theory and the words \"network, node, link\" are used more in Social Network Analysis concepts. Since this isn't a course in Graph Theory or Social Network Analysis, there is no problem using the one term instead of the other.\n",
    "\n",
    "## 2. Datasets\n",
    "--------------------------------\n",
    "\n",
    "In this section we describe the datasets used and how they were obtained. Since in real life we don't get to work with perfect data gently handled to us, we also chose to try working not only with toy datasets but to also create our own. For the dataset we create ourselves, we state explicitly how it was created (and present the code to create it) and for the one found online, we state how we aquired it and include it in the data folder.\n",
    "\n",
    "### 2.A Followers on Github\n",
    "\n",
    "The nodes in this graph will be Github profiles and the edges will be \"follower\" relationships. An edge from user $A$ to user $B$ means that $A$ follows $B$. To create the graph we start with a Github account (in this case mine) and find all of its neighbours, that is users this account follows or users that follow this account. Then we do the same for the neighbours of the first node and continue to work this way for $N=3$ iterations. This gives us:\n",
    "\n",
    "- 17 nodes in the first step (followers or following \"michalisvaz\")\n",
    "- 115 nodes in the second step (followers or following some of the previous 17 nodes but not the initial node)\n",
    "- 2557 nodes in the third step (working the same way)\n",
    "\n",
    "However for the 2557 nodes of the last step we haven't kept track of their neighbors (followers and following). So for these nodes we add as neighbors the (followers and following) nodes that are already in our graph. Of course there are millions of other users on Github, but if we didn't stop at some point, the graph would be huge to run PageRank on it.\n",
    "\n",
    "Bellow is the code used for this task. Note that if you run this code at another time in the future, you may get different numbers since users may follow or unfollow other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Number of nodes to add their neighbours:  1\n",
      "Time it took:  1.3589646816253662  seconds\n",
      "-----------------------------\n",
      "Number of nodes to add their neighbours:  17\n",
      "Time it took:  21.387369871139526  seconds\n",
      "-----------------------------\n",
      "Number of nodes to add their neighbours:  115\n",
      "Time it took:  187.46242809295654  seconds\n",
      "-----------------------------\n",
      "Adding links to existing nodes for the last  2557  nodes\n",
      "Time it took:  3735.7351458072662  seconds\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, re, argparse, sys, time\n",
    "\n",
    "# gets links to other accounts from a page\n",
    "def get_links_to(soup):\n",
    "    nodes_to = set(soup.find_all(\"span\", {\"class\": \"Link--secondary pl-1\"})) | set(soup.find_all(\"span\", {\"class\": \"Link--secondary\"}))\n",
    "    to_return = set()\n",
    "    for x in nodes_to:\n",
    "        y = str(x).split(\">\", 1)[1].split(\"<\", 1)[0]\n",
    "        new_nodes.add(y)\n",
    "        to_return.add(y)\n",
    "    return to_return\n",
    "\n",
    "N = 3\n",
    "nodes = {\"michalisvaz\"}\n",
    "current_nodes = {\"michalisvaz\"}\n",
    "default_url = \"https://github.com/\"\n",
    "tab_followers = \"?tab=followers\"\n",
    "tab_following = \"?tab=following\"\n",
    "graph = dict()\n",
    "print(\"-----------------------------\")\n",
    "for i in range(N):\n",
    "    start = time.time()\n",
    "    new_nodes = set()\n",
    "    print(\"Number of nodes to add their neighbours: \", len(current_nodes))\n",
    "    for node in current_nodes:\n",
    "        followers_url = default_url + node + tab_followers\n",
    "        r = requests.get(followers_url)\n",
    "        html = r.content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        temp1 = get_links_to(soup)\n",
    "        following_url = default_url + node + tab_following\n",
    "        r = requests.get(following_url)\n",
    "        html = r.content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        temp2 = get_links_to(soup)\n",
    "        graph[node] = (temp1, temp2)\n",
    "    new_nodes = new_nodes - nodes\n",
    "    nodes = nodes | new_nodes\n",
    "    current_nodes = new_nodes\n",
    "    end = time.time()\n",
    "    print(\"Time it took: \", end - start, \" seconds\")\n",
    "    print(\"-----------------------------\")\n",
    "\n",
    "# gets links to already existing nodes of the graph\n",
    "def get_links_to_existing_nodes(soup):\n",
    "    nodes_to = set(soup.find_all(\"span\", {\"class\": \"Link--secondary pl-1\"})) | set(soup.find_all(\"span\", {\"class\": \"Link--secondary\"}))\n",
    "    to_return = set()\n",
    "    for x in nodes_to:\n",
    "        y = str(x).split(\">\", 1)[1].split(\"<\", 1)[0]\n",
    "        if y in nodes:\n",
    "            to_return.add(y)\n",
    "    return to_return\n",
    "\n",
    "start = time.time()\n",
    "print(\"Adding links to existing nodes for the last \", len(current_nodes), \" nodes\")\n",
    "for node in current_nodes:\n",
    "    followers_url = default_url + node + tab_followers\n",
    "    r = requests.get(followers_url)\n",
    "    html = r.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    temp1 = get_links_to_existing_nodes(soup)\n",
    "    following_url = default_url + node + tab_following\n",
    "    r = requests.get(following_url)\n",
    "    html = r.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    temp2 = get_links_to_existing_nodes(soup)\n",
    "    graph[node] = (temp1, temp2)\n",
    "end = time.time()\n",
    "print(\"Time it took: \", end - start, \" seconds\")\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards we match each node name to an id and save the graph to an edgelist of ids. We also create a file to keep the matchings from id to node name and from node name to id stored. We don't need two separate files for the matching since the matching is 1-1. We also don't need to keep track of both followers and following relationships since for each edge \"$B$ has $A$ as a follower\" there exists an edge \"$A$ follows $B$\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 1\n",
    "names2ids = dict()\n",
    "for node in nodes:\n",
    "    names2ids[node] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "edges = set()\n",
    "for head in graph:\n",
    "    for tail in graph[head][1]:\n",
    "        edge = (names2ids[head], names2ids[tail])\n",
    "        edges.add(edge)\n",
    "        \n",
    "with open(\"data/github_users/id_edgelist.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for edge in edges:\n",
    "        f.write(str(edge[0]) + \",\" + str(edge[1]) + \"\\n\")\n",
    "with open(\"data/github_users/matchings.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for name in names2ids:\n",
    "        f.write(name + \",\" + str(names2ids[name]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below we can see the number of edges and vertices in the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices:  2690\n",
      "Number of edges:  8402\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of vertices: \", len(nodes))\n",
    "print(\"Number of edges: \", len(edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B Philosopher Network\n",
    "\n",
    "This dataset is a graph where the vertices are philosophers and an edge from philosopher $h$ to philosopher $t$ means that $h$'s work infuenced $t$'s work. This graph was handled to us in an edgelist form for a project for the course \"Algorithms\" (of the Department of Informatics), and can be found under the folder `data/philosophers` with the name `philosophy_edgelist.txt`. One file with the graph as a node-id edgelist, and one with the names-ids matching were also added to this folder (working the same way as with the previous dataset).\n",
    "\n",
    "Below is the code for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original = pd.read_csv(\"data/philosophers/philosophy_edgelist.txt\", \n",
    "                       sep=\"\\t\", header=None, names=[\"source name\", \"target name\"])\n",
    "nodes = set(original[\"source name\"]) | set(original[\"target name\"])\n",
    "\n",
    "cnt = 1\n",
    "names2ids = dict()\n",
    "for node in nodes:\n",
    "    names2ids[node] = cnt\n",
    "    cnt += 1\n",
    "    \n",
    "original[\"source id\"] = original[\"source name\"].map(names2ids)\n",
    "original[\"target id\"] = original[\"target name\"].map(names2ids)\n",
    "\n",
    "original[[\"source id\", \"target id\"]].to_csv(\"data/philosophers/id_edgelist.txt\", header=False, index=False)\n",
    "with open(\"data/philosophers/matchings.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for name in names2ids:\n",
    "        f.write(name + \",\" + str(names2ids[name]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the number of vertices and edges for the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices:  1231\n",
      "Number of edges:  7303\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of vertices: \", len(nodes))\n",
    "print(\"Number of edges: \", original.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics of PageRank\n",
    "--------------------------------\n",
    "\n",
    "The main idea of PageRank is to calculate a score for each page which represents the probability that a person randomly clicking on links will arrive at the certain page. This probability is the PageRank score of the page. If we are talking about graphs in general and not necessarily web pages, then the nodes of the graph are the equivalent of the pages and the edges of the graph are the equivalent of hyperlinks to other pages. We assume that every link in a page is clicked with equal probability (or equivallently for graphs that a user walking randomly across the graph, always chooses a neighbor of their current location(node) uniformly). Based on this the PageRank scores $PR(u)$ for every node $u$ are:\n",
    "\n",
    "$$PR(u) = \\sum_{v \\in B(u)} \\frac{PR(v)}{L(v)}$$\n",
    "\n",
    ", where $B(u)$ is the set of nodes $v$ for which there exists an edge $(v, u)$ and $L(v)$ is the out-degree of a node.\n",
    "\n",
    "In practice we assume that a user who randomly clicks on links will eventually stop clicking. We denote the probability that a user **continues** clicking as $d$ (damping factor) and the PageRank scores become:\n",
    "\n",
    "$$PR(u) = \\frac{1-d}{N} + d\\sum_{v \\in B(u)} \\frac{PR(v)}{L(v)}$$\n",
    "\n",
    ", where $N$ is the number of nodes in the graph. The damping has been empirically calculated to be around $0.85$ for web pages.\n",
    "\n",
    "Here we should note that for sinks, which are the pages with no outbound edges, we assume that they have a link to every other page. This is done because otherwise the non-sink pages would be at a huge disadvantage.\n",
    "\n",
    "### Computation of Pagerank Scores\n",
    "\n",
    "The PageRank scores can be calculated either iteratively or algebraically. Below we present the two methods.\n",
    "\n",
    "#### Iterative method\n",
    "\n",
    "At the start of the iterative method, that is at $t=0$, we assume a uniform probability distribution on the nodes of the graph. So we have:\n",
    "\n",
    "$$PR(u_i; 0) = \\frac{1}{N}$$\n",
    "\n",
    ", where $N$ is the number of nodes and $PR(u_i; 0)$ is the PageRank score of node $u_i$ at time $t=0$.\n",
    "\n",
    "At each step of the iteration the PageRank scores are updated by the following formula (based on the rule written above):\n",
    "\n",
    "$$PR(u_i; t+1) = \\frac{1-d}{N} + d \\sum_{u_j \\in B(u_i)} \\frac{PR(u_j; t)}{L(u_j)}$$\n",
    "\n",
    "where $L(u_j)$ is the number of outbound links from node $u_j$ and $d$ is the damping factor.\n",
    "\n",
    "The formula can be written in matrix notation as:\n",
    "\n",
    "$$R(t+1) = dMR(t) + \\frac{1-d}{N}\\textbf{1} \\quad \\quad \\quad (1)$$\n",
    "\n",
    ", where $R_i(t) = PR(u_i; t)$, $\\textbf{1}$ is the column vector of length $N$ containing only ones and $M$ is defined as:\n",
    "\n",
    "$$\n",
    "M = \n",
    "\\begin{cases}\n",
    "1/L(u_j),  & \\text{if $j$ links to $i$} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The computations stops when the method has converged, that is when:\n",
    "\n",
    "$$|R(t+1) - R(t)| < \\epsilon$$\n",
    "\n",
    "for some small $\\epsilon$.\n",
    "\n",
    "#### Algebraic method\n",
    "\n",
    "For $t \\rightarrow \\infty$ (when the iterative method has converged) the equation $(1)$ becomes:\n",
    "\n",
    "$$R = dMR + \\frac{1-d}{N}\\textbf{1}$$\n",
    "\n",
    "We can solve this system and get:\n",
    "\n",
    "$$R = (\\textbf{I} - dM)^{-1} \\frac{1-d}{N} \\textbf{1}$$\n",
    "\n",
    ", where $\\textbf{I}$ is the identity matrix. It can be proven that for $d \\in (0, 1)$ the solution exists and is unique.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "--------------------------------\n",
    "\n",
    "Below there is an implementation of both the Iterative and the Algebraic method for calculating PageRank. For the algebraic method we check that $0 < d < 1$ so that we don't have problems with existance and uniqueness of solution of the system. For the Iterative Method we pass as an argument to the function a number `epsilon` which is the treshold for the value $|R(t+1) - R(t)|$ to be smaller than it, in order for the method to stop. We also pass the maximum number of iterations to perform. This means that if we have performed more than `max_iter` iterations we stop regardless of having achieved convergence or not. Both functions assume that the sum of each column of the adjacency matrix sums to one.\n",
    "\n",
    "After the implementation we test our algorithm with a random $1000 \\times 1000$ adjacency matrix. After running both versions we print the Pearson's correlation coefficient between the results of the two algorithms. If our implementations are correct (or if they have the same mistake) the coefficient must be very close to $1$. And we can see that indeed it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running algebraic version of PageRank...\n",
      "Running iterative version of PageRank...\n",
      "Converged after  3  iterations\n",
      "Correlation coefficient between results with the two methods: \n",
      "0.9999999998045176\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The algebraic implementation of PageRank\n",
    "def PageRank_algebraic(M, d=0.85):\n",
    "    if d<=0 or d>=1:\n",
    "        print(\"d must be greater than 0 and less than 1\")\n",
    "        return None\n",
    "    if M.shape[0] != M.shape[1] or len(M.shape)!=2:\n",
    "        print(\"M must be a square matrix\")\n",
    "        return None\n",
    "    N = M.shape[0]\n",
    "    ones = np.ones((N, 1))\n",
    "    I = np.identity(N)\n",
    "    return np.matmul(np.linalg.inv(I - d*M) * ((1-d)/N), ones)\n",
    "\n",
    "# The iterative implementation of PageRank\n",
    "def PageRank_iterative(M, d=0.85, epsilon=1e-5, max_iter=100):\n",
    "    if M.shape[0] != M.shape[1] or len(M.shape)!=2:\n",
    "        print(\"M must be a square matrix\")\n",
    "        return None\n",
    "    N = M.shape[0]\n",
    "    ones = np.ones((N, 1))\n",
    "    R_current = ones/N\n",
    "    for i in range(max_iter):\n",
    "        R_prev = R_current\n",
    "        R_current = np.matmul(d*M, R_prev) + ((1-d)/N)*ones\n",
    "        if np.linalg.norm(R_current-R_prev) < epsilon:\n",
    "            print(\"Converged after \", i+1, \" iterations\")\n",
    "            return R_current\n",
    "    print(\"Didn't converge. Stopped after \", max_iter, \" iterations\")\n",
    "    return R_current\n",
    "        \n",
    "\n",
    "# Regularizes a square matrix so that each column sums to 1. \n",
    "# Usually when we say that a matrix is stochastic we mean that \n",
    "# its rows sum to 1. Here we need the columns to sum to 1\n",
    "def make_stochastic(M):\n",
    "    dimension = M.shape[0]\n",
    "    if M.shape[0] != M.shape[1] or len(M.shape)!=2:\n",
    "        print(\"M must be a square matrix\")\n",
    "        return None\n",
    "    for c in range(dimension):\n",
    "        if M[:, c].sum()==0:\n",
    "            M[:, c] = np.ones(dimension)/dimension\n",
    "    M = M / M.sum(axis=0)\n",
    "    return M\n",
    "\n",
    "dimension = 1000\n",
    "d = 0.85\n",
    "M = np.random.randint(2, size=(dimension, dimension))\n",
    "M = make_stochastic(M)\n",
    "print(\"Running algebraic version of PageRank...\")\n",
    "results_alg = PageRank_algebraic(M, d)\n",
    "# print(\"Results: \")\n",
    "# print(results_alg)\n",
    "print(\"Running iterative version of PageRank...\")\n",
    "results_iter = PageRank_iterative(M, d)\n",
    "# print(\"Results: \")\n",
    "# print(results_iter)\n",
    "print(\"Correlation coefficient between results with the two methods: \")\n",
    "print(np.corrcoef(results_alg.flatten(), results_iter.flatten())[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "--------------------------------\n",
    "\n",
    "In this section we'll test our implementation of PageRank in the two datasets specified above. For each dataset we'll present the top $k=10$ nodes (in the first case nodes are Github users and in the second case, philosophers). We will also check to what extent the results of PageRank correlate with other measures of node importance in a graph. To calculate these other metrics we'll use `networkX`, a Python library for the creation, manipulation, and study of complex networks. The metrics we'll compare PageRank with, are In-Degree Centrality, Closeness Centrality, Betweenness Centrality and Eigenvector Centrality.\n",
    "\n",
    "### Followers on Github\n",
    "\n",
    "Let's start with the \"Followers on Github\" graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running algebraic version of PageRank...\n",
      "Running iterative version of PageRank...\n",
      "Converged after  30  iterations\n",
      "Correlation coefficient between results with the two methods: \n",
      "0.9999998681753948\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "edgelist_df = pd.read_csv(\"data/github_users/id_edgelist.txt\", \n",
    "                       sep=\",\", header=None, names=[\"source id\", \"target id\"])\n",
    "edgelist = edgelist_df.to_numpy()\n",
    "adj_matrix = np.zeros((edgelist.max(), edgelist.max()))\n",
    "for e in edgelist:\n",
    "    adj_matrix[e[1]-1][e[0]-1] = 1\n",
    "    \n",
    "d = 0.85\n",
    "adj_matrix = make_stochastic(adj_matrix)\n",
    "print(\"Running algebraic version of PageRank...\")\n",
    "results_alg = PageRank_algebraic(adj_matrix, d)\n",
    "# print(\"Results: \")\n",
    "# print(results_alg)\n",
    "print(\"Running iterative version of PageRank...\")\n",
    "results_iter = PageRank_iterative(adj_matrix, d)\n",
    "# print(\"Results: \")\n",
    "# print(results_iter)\n",
    "print(\"Correlation coefficient between results with the two methods: \")\n",
    "print(np.corrcoef(results_alg.flatten(), results_iter.flatten())[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print the top nodes of our graph based in PageRank scores (we'll use the results from the algebraic method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>id</th>\n",
       "      <th>PageRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>JakeWharton</td>\n",
       "      <td>1218</td>\n",
       "      <td>0.016931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>torvalds</td>\n",
       "      <td>1915</td>\n",
       "      <td>0.014418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>dspinellis</td>\n",
       "      <td>2668</td>\n",
       "      <td>0.009126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>nat</td>\n",
       "      <td>1106</td>\n",
       "      <td>0.008671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>buckyroberts</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.006919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>chrisbanes</td>\n",
       "      <td>530</td>\n",
       "      <td>0.006801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>gaearon</td>\n",
       "      <td>2596</td>\n",
       "      <td>0.006373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>mitchtabian</td>\n",
       "      <td>820</td>\n",
       "      <td>0.005881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>JideGuru</td>\n",
       "      <td>1625</td>\n",
       "      <td>0.005788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>yyx990803</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.005649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username    id  PageRank\n",
       "1217   JakeWharton  1218  0.016931\n",
       "1914      torvalds  1915  0.014418\n",
       "2667    dspinellis  2668  0.009126\n",
       "1105           nat  1106  0.008671\n",
       "1737  buckyroberts  1738  0.006919\n",
       "529     chrisbanes   530  0.006801\n",
       "2595       gaearon  2596  0.006373\n",
       "819    mitchtabian   820  0.005881\n",
       "1624      JideGuru  1625  0.005788\n",
       "1674     yyx990803  1675  0.005649"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_df = pd.read_csv(\"data/github_users/matchings.txt\", \n",
    "                       sep=\",\", header=None, names=[\"username\", \"id\"])\n",
    "nodes_df[\"PageRank\"] = results_alg.flatten()\n",
    "nodes_df.sort_values(by=\"PageRank\", ascending=False, inplace=True)\n",
    "nodes_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that among the top results are some of the most followed users of Github and also that professor Diomidis Spinellis appears on the top nodes, since he is likely to be followed by nodes close to my profile (probably other AUEB students). Of course, him having a popular Github account with many contributions, plays a vital role as well.\n",
    "\n",
    "Now we'll check how the PageRank scores for this graph compare with other metrics about the importance of a node. We'll use `networkX` to calculate these metrics and then we'll see the correlation coefficients between PageRank and each of the other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between PageRank and:\n",
      "In-degree Centrality :  0.9221225738999792\n",
      "Closeness Centrality :  0.46330017420505026\n",
      "Betweenness Centrality :  0.30749575416600966\n",
      "Eigenvector Centrality :  0.259704009381689\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "number_of_nodes = edgelist.max()\n",
    "github_graph = nx.DiGraph()\n",
    "github_graph.add_edges_from(edgelist)\n",
    "flat_results = results_alg.flatten()\n",
    "print(\"Correlation between PageRank and:\")\n",
    "# In degree\n",
    "in_degree_dict = nx.in_degree_centrality(github_graph)\n",
    "in_degree_centrality = np.zeros(number_of_nodes)\n",
    "for x in in_degree_dict:\n",
    "    in_degree_centrality[x-1] = in_degree_dict[x]\n",
    "print(\"In-degree Centrality : \", np.corrcoef(flat_results, in_degree_centrality)[0][1])\n",
    "# Closeness Centrality\n",
    "closeness_dict = nx.closeness_centrality(github_graph)\n",
    "closeness_centrality = np.zeros(number_of_nodes)\n",
    "for x in closeness_dict:\n",
    "    closeness_centrality[x-1] = closeness_dict[x]\n",
    "print(\"Closeness Centrality : \", np.corrcoef(flat_results, closeness_centrality)[0][1])\n",
    "# Betweenness Centrality\n",
    "betweenness_dict = nx.betweenness_centrality(github_graph)\n",
    "betweenness_centrality = np.zeros(number_of_nodes)\n",
    "for x in betweenness_dict:\n",
    "    betweenness_centrality[x-1] = betweenness_dict[x]\n",
    "print(\"Betweenness Centrality : \", np.corrcoef(flat_results, betweenness_centrality)[0][1])\n",
    "# Eigenvector Centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(github_graph)\n",
    "eigenvector_centrality = np.zeros(number_of_nodes)\n",
    "for x in eigenvector_dict:\n",
    "    eigenvector_centrality[x-1] = eigenvector_dict[x]\n",
    "print(\"Eigenvector Centrality : \", np.corrcoef(flat_results, eigenvector_centrality)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Philosopher Network\n",
    "\n",
    "We'll now do the same for the philosopher network. However, the most important philosophers aren't the ones which **were influenced** by many and/or important philosophers but ones who **influenced** many and/or important other philosophers. So, we need to take a graph with the reverse of each edge to run our implementation of PageRank on.\n",
    "\n",
    "We do this and run our two PageRank implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running algebraic version of PageRank...\n",
      "Running iterative version of PageRank...\n",
      "Converged after  15  iterations\n",
      "Correlation coefficient between results with the two methods: \n",
      "0.9999999519454222\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "edgelist_df = pd.read_csv(\"data/philosophers/id_edgelist.txt\", \n",
    "                       sep=\",\", header=None, names=[\"source id\", \"target id\"])\n",
    "edgelist = edgelist_df.to_numpy()\n",
    "adj_matrix = np.zeros((edgelist.max(), edgelist.max()))\n",
    "for e in edgelist:\n",
    "    adj_matrix[e[0]-1][e[1]-1] = 1\n",
    "    \n",
    "d = 0.85\n",
    "adj_matrix = make_stochastic(adj_matrix)\n",
    "print(\"Running algebraic version of PageRank...\")\n",
    "results_alg = PageRank_algebraic(adj_matrix, d)\n",
    "# print(\"Results: \")\n",
    "# print(results_alg)\n",
    "print(\"Running iterative version of PageRank...\")\n",
    "results_iter = PageRank_iterative(adj_matrix, d)\n",
    "# print(\"Results: \")\n",
    "# print(results_iter)\n",
    "print(\"Correlation coefficient between results with the two methods: \")\n",
    "print(np.corrcoef(results_alg.flatten(), results_iter.flatten())[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we'll print the top nodes of our graph based in PageRank scores (as before, we'll use the results from the algebraic method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>philosopher</th>\n",
       "      <th>id</th>\n",
       "      <th>PageRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>aristotle</td>\n",
       "      <td>666</td>\n",
       "      <td>0.025997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>plato</td>\n",
       "      <td>311</td>\n",
       "      <td>0.019387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>immanuel_kant</td>\n",
       "      <td>987</td>\n",
       "      <td>0.014274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>john_locke</td>\n",
       "      <td>747</td>\n",
       "      <td>0.009887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>bertrand_russell</td>\n",
       "      <td>376</td>\n",
       "      <td>0.009865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>thomas_aquinas</td>\n",
       "      <td>1189</td>\n",
       "      <td>0.009309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>cicero</td>\n",
       "      <td>990</td>\n",
       "      <td>0.008775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>rené_descartes</td>\n",
       "      <td>100</td>\n",
       "      <td>0.008181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>david_hume</td>\n",
       "      <td>716</td>\n",
       "      <td>0.008140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>friedrich_nietzsche</td>\n",
       "      <td>570</td>\n",
       "      <td>0.007996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              philosopher    id  PageRank\n",
       "665             aristotle   666  0.025997\n",
       "310                 plato   311  0.019387\n",
       "986         immanuel_kant   987  0.014274\n",
       "746            john_locke   747  0.009887\n",
       "375      bertrand_russell   376  0.009865\n",
       "1188       thomas_aquinas  1189  0.009309\n",
       "989                cicero   990  0.008775\n",
       "99         rené_descartes   100  0.008181\n",
       "715            david_hume   716  0.008140\n",
       "569   friedrich_nietzsche   570  0.007996"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_df = pd.read_csv(\"data/philosophers/matchings.txt\", \n",
    "                       sep=\",\", header=None, names=[\"philosopher\", \"id\"])\n",
    "nodes_df[\"PageRank\"] = results_alg.flatten()\n",
    "nodes_df.sort_values(by=\"PageRank\", ascending=False, inplace=True)\n",
    "nodes_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we'll check the strengh of the relationship between PageRank and other Centrality metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between PageRank and:\n",
      "In-degree Centrality :  0.6593748879083564\n",
      "Closeness Centrality :  0.24647552993932617\n",
      "Betweenness Centrality :  0.8809129230695634\n",
      "Eigenvector Centrality :  0.5483149457490674\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "number_of_nodes = edgelist.max()\n",
    "philosophers_graph = nx.DiGraph()\n",
    "philosophers_graph.add_edges_from(edgelist)\n",
    "flat_results = results_alg.flatten()\n",
    "print(\"Correlation between PageRank and:\")\n",
    "# In degree\n",
    "in_degree_dict = nx.in_degree_centrality(philosophers_graph)\n",
    "in_degree_centrality = np.zeros(number_of_nodes)\n",
    "for x in in_degree_dict:\n",
    "    in_degree_centrality[x-1] = in_degree_dict[x]\n",
    "print(\"In-degree Centrality : \", np.corrcoef(flat_results, in_degree_centrality)[0][1])\n",
    "# Closeness Centrality\n",
    "closeness_dict = nx.closeness_centrality(philosophers_graph)\n",
    "closeness_centrality = np.zeros(number_of_nodes)\n",
    "for x in closeness_dict:\n",
    "    closeness_centrality[x-1] = closeness_dict[x]\n",
    "print(\"Closeness Centrality : \", np.corrcoef(flat_results, closeness_centrality)[0][1])\n",
    "# Betweenness Centrality\n",
    "betweenness_dict = nx.betweenness_centrality(philosophers_graph)\n",
    "betweenness_centrality = np.zeros(number_of_nodes)\n",
    "for x in betweenness_dict:\n",
    "    betweenness_centrality[x-1] = betweenness_dict[x]\n",
    "print(\"Betweenness Centrality : \", np.corrcoef(flat_results, betweenness_centrality)[0][1])\n",
    "# Eigenvector Centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(philosophers_graph)\n",
    "eigenvector_centrality = np.zeros(number_of_nodes)\n",
    "for x in eigenvector_dict:\n",
    "    eigenvector_centrality[x-1] = eigenvector_dict[x]\n",
    "print(\"Eigenvector Centrality : \", np.corrcoef(flat_results, eigenvector_centrality)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on results & final thoughts\n",
    "--------------------------------\n",
    "\n",
    "We can see that even though PageRank doesn't value all links equally, it still has a very high correlation with the In-degree Centrality for both graphs used for our experiments. The Closeness Centrality has a lower Pearson's coefficient with PageRank but surely an important one. \n",
    "\n",
    "The coefficients between PageRank and Betweenness Centrality and between PageRank and Eigenvector Centrality are much stronger in the second graph. This is strange, but it may occur due to the second graph being denser or due to a certain particularity in the structure of one of the graphs.\n",
    "\n",
    "As stated in the introduction, PageRank by itself can't get us very far when we try to extract the best search results for a term, most importantly because it doesn't take into account the term searched or the content of each webpage (node). However, it still is a very usefull tool for information retrieval from graph-like structures and understanding it may serve as a first step for someone wanting to dive deeper into information retrieval and into the way search engines work.\n",
    "\n",
    "## Sources\n",
    "\n",
    "* Wikipedia\n",
    "* A survey of eigenvector methods for Web Information Retrieval (Amy N. Langville & Carl D. Meyer)\n",
    "* PageRank Beyond the Web (David F. Gleich)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
